{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "LfRsA9WesyMJ",
    "outputId": "77f1e817-48a9-4e3c-9974-90d6deee479e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current Device: 0\n",
      "Device Name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Current Device:\", torch.cuda.current_device())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQui8Qwsk-Ct"
   },
   "source": [
    "## Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aTJWLGTNk6E2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzftdIj2lC1Z"
   },
   "source": [
    "## Load & Preprocess IP102 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdOA6vS3lYW0"
   },
   "source": [
    "# ResNet50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Path\n",
    "dataset_path = r'C:\\Users\\merye\\Desktop\\IP102_Wheat_Cereal_Pests'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCPPtSxlI0gW",
    "outputId": "bef9f3a7-f890-4133-a486-d7f178f993cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 2.1917 | Acc: 0.3995 | Time: 228.36s\n",
      "Val Loss: 1.6316 | Acc: 0.6005 | Time: 24.43s\n",
      "Epoch 2/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.6073 | Acc: 0.6141 | Time: 161.14s\n",
      "Val Loss: 1.3160 | Acc: 0.7185 | Time: 20.63s\n",
      "Epoch 3/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.4071 | Acc: 0.6774 | Time: 210.15s\n",
      "Val Loss: 1.2270 | Acc: 0.7493 | Time: 21.65s\n",
      "Epoch 4/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.3135 | Acc: 0.7216 | Time: 225.42s\n",
      "Val Loss: 1.2424 | Acc: 0.7265 | Time: 19.93s\n",
      "Epoch 5/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.1921 | Acc: 0.7679 | Time: 222.60s\n",
      "Val Loss: 1.1358 | Acc: 0.7922 | Time: 21.72s\n",
      "Epoch 6/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.1231 | Acc: 0.7942 | Time: 220.98s\n",
      "Val Loss: 1.1308 | Acc: 0.7788 | Time: 17.79s\n",
      "Epoch 7/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.0323 | Acc: 0.8417 | Time: 217.45s\n",
      "Val Loss: 1.0960 | Acc: 0.8204 | Time: 24.30s\n",
      "Epoch 8/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.0049 | Acc: 0.8473 | Time: 226.39s\n",
      "Val Loss: 1.0767 | Acc: 0.8110 | Time: 24.08s\n",
      "Epoch 9/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9508 | Acc: 0.8705 | Time: 225.71s\n",
      "Val Loss: 1.0699 | Acc: 0.8137 | Time: 19.82s\n",
      "Epoch 10/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9374 | Acc: 0.8749 | Time: 219.72s\n",
      "Val Loss: 1.0685 | Acc: 0.8150 | Time: 20.18s\n",
      "Early stopping triggered! Training halted.\n",
      "Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "# Data transformation for training and validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Paths and loading data\n",
    "data_dir = 'C:/Users/merye/Desktop/IP102_Wheat_Cereal_Pests'\n",
    "image_datasets = {x: datasets.ImageFolder(root=f\"{data_dir}/{x}\", transform=data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=2) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Device setup (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ResNet50 model with customized fully connected layer\n",
    "def create_model():\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.6),\n",
    "        nn.Linear(num_ftrs, num_classes)\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# Training loop with early stopping\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30, patience=3):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}\\n' + '-' * 50)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss, running_corrects = 0.0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f} | Time: {elapsed_time:.2f}s')\n",
    "\n",
    "            if phase == 'val':\n",
    "                scheduler.step()\n",
    "\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    early_stop_counter = 0\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter >= patience:\n",
    "                        print(\"Early stopping triggered! Training halted.\")\n",
    "                        model.load_state_dict(best_model_wts)\n",
    "                        return model\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Initialize model, loss, optimizer, and scheduler\n",
    "model = create_model()\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "\n",
    "# Train model\n",
    "model = train_model(model, criterion, optimizer, scheduler, num_epochs=30, patience=3)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"resnet50_finetuned.pth\")\n",
    "print(\"Model training complete and saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AAHAdufuBKg"
   },
   "source": [
    "# EfficientB2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8uDYqZlJO8N",
    "outputId": "5b1e1591-f338-4375-f6c1-88d3d1ad3942"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 2.6070 | Acc: 0.2347 | Time: 42.54s\n",
      "Val Loss: 2.0817 | Acc: 0.4718 | Time: 8.40s\n",
      "Epoch 2/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 2.0432 | Acc: 0.4672 | Time: 42.42s\n",
      "Val Loss: 1.5993 | Acc: 0.6582 | Time: 6.67s\n",
      "Epoch 3/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.7280 | Acc: 0.5716 | Time: 39.96s\n",
      "Val Loss: 1.3494 | Acc: 0.7185 | Time: 7.00s\n",
      "Epoch 4/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.5279 | Acc: 0.6438 | Time: 53.71s\n",
      "Val Loss: 1.2479 | Acc: 0.7359 | Time: 7.54s\n",
      "Epoch 5/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.4051 | Acc: 0.6919 | Time: 79.03s\n",
      "Val Loss: 1.1816 | Acc: 0.7694 | Time: 8.01s\n",
      "Epoch 6/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.3250 | Acc: 0.7274 | Time: 65.37s\n",
      "Val Loss: 1.1343 | Acc: 0.7922 | Time: 7.51s\n",
      "Epoch 7/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.2663 | Acc: 0.7539 | Time: 73.73s\n",
      "Val Loss: 1.1082 | Acc: 0.7936 | Time: 7.43s\n",
      "Epoch 8/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.2373 | Acc: 0.7584 | Time: 79.12s\n",
      "Val Loss: 1.0977 | Acc: 0.7949 | Time: 7.40s\n",
      "Epoch 9/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.1940 | Acc: 0.7702 | Time: 63.62s\n",
      "Val Loss: 1.0903 | Acc: 0.7989 | Time: 7.97s\n",
      "Epoch 10/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.1428 | Acc: 0.7938 | Time: 81.61s\n",
      "Val Loss: 1.0860 | Acc: 0.7949 | Time: 7.19s\n",
      "Epoch 11/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.1174 | Acc: 0.8036 | Time: 59.19s\n",
      "Val Loss: 1.0739 | Acc: 0.8097 | Time: 7.17s\n",
      "Epoch 12/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.0964 | Acc: 0.8125 | Time: 79.08s\n",
      "Val Loss: 1.0790 | Acc: 0.8083 | Time: 7.02s\n",
      "Epoch 13/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.0771 | Acc: 0.8230 | Time: 61.49s\n",
      "Val Loss: 1.0814 | Acc: 0.8029 | Time: 7.09s\n",
      "Epoch 14/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.0513 | Acc: 0.8299 | Time: 78.51s\n",
      "Val Loss: 1.0730 | Acc: 0.8056 | Time: 8.77s\n",
      "Epoch 15/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.0318 | Acc: 0.8350 | Time: 67.18s\n",
      "Val Loss: 1.0681 | Acc: 0.8177 | Time: 7.15s\n",
      "Epoch 16/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.0070 | Acc: 0.8502 | Time: 73.42s\n",
      "Val Loss: 1.0681 | Acc: 0.8097 | Time: 7.22s\n",
      "Epoch 17/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 1.0047 | Acc: 0.8527 | Time: 78.48s\n",
      "Val Loss: 1.0779 | Acc: 0.8043 | Time: 7.14s\n",
      "Epoch 18/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9873 | Acc: 0.8613 | Time: 59.71s\n",
      "Val Loss: 1.0782 | Acc: 0.8083 | Time: 7.19s\n",
      "Epoch 19/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9844 | Acc: 0.8564 | Time: 78.66s\n",
      "Val Loss: 1.0761 | Acc: 0.8177 | Time: 7.18s\n",
      "Epoch 20/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9776 | Acc: 0.8607 | Time: 57.39s\n",
      "Val Loss: 1.0812 | Acc: 0.8231 | Time: 7.44s\n",
      "Epoch 21/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9700 | Acc: 0.8634 | Time: 80.37s\n",
      "Val Loss: 1.0731 | Acc: 0.8271 | Time: 7.51s\n",
      "Epoch 22/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9703 | Acc: 0.8712 | Time: 100.95s\n",
      "Val Loss: 1.0685 | Acc: 0.8244 | Time: 15.42s\n",
      "Epoch 23/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9491 | Acc: 0.8761 | Time: 152.35s\n",
      "Val Loss: 1.0736 | Acc: 0.8177 | Time: 15.73s\n",
      "Epoch 24/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9641 | Acc: 0.8645 | Time: 146.13s\n",
      "Val Loss: 1.0637 | Acc: 0.8271 | Time: 11.95s\n",
      "Epoch 25/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9543 | Acc: 0.8680 | Time: 136.47s\n",
      "Val Loss: 1.0702 | Acc: 0.8284 | Time: 12.09s\n",
      "Epoch 26/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9381 | Acc: 0.8810 | Time: 129.59s\n",
      "Val Loss: 1.0701 | Acc: 0.8204 | Time: 14.46s\n",
      "Epoch 27/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9372 | Acc: 0.8810 | Time: 129.22s\n",
      "Val Loss: 1.0658 | Acc: 0.8204 | Time: 11.58s\n",
      "Epoch 28/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9385 | Acc: 0.8734 | Time: 132.06s\n",
      "Val Loss: 1.0677 | Acc: 0.8298 | Time: 10.55s\n",
      "Epoch 29/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9570 | Acc: 0.8714 | Time: 127.98s\n",
      "Val Loss: 1.0665 | Acc: 0.8284 | Time: 14.03s\n",
      "Epoch 30/30\n",
      "--------------------------------------------------\n",
      "Train Loss: 0.9343 | Acc: 0.8803 | Time: 155.11s\n",
      "Val Loss: 1.0787 | Acc: 0.8177 | Time: 13.93s\n",
      "Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "# Load Pretrained EfficientNetB2 and Modify\n",
    "def create_model():\n",
    "    model = models.efficientnet_b2(weights=\"DEFAULT\")  # EfficientNetB2\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.6),\n",
    "        nn.Linear(num_ftrs, num_classes)\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# Training function with Early Stopping\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30, patience=5):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}\\n' + '-' * 50)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss, running_corrects = 0.0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f} | Time: {elapsed_time:.2f}s')\n",
    "\n",
    "            if phase == 'val':\n",
    "                scheduler.step()\n",
    "\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    early_stop_counter = 0  # Reset early stopping\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter >= patience:\n",
    "                        print(\"Early stopping triggered! Best model restored.\")\n",
    "                        model.load_state_dict(best_model_wts)\n",
    "                        return model\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Initialize model, loss, optimizer, and scheduler\n",
    "model = create_model()\n",
    "\n",
    "# Loss function with label smoothing and CrossEntropy\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Optimizer with reduced learning rate and weight decay\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-5)\n",
    "\n",
    "# Cosine Annealing scheduler with adjusted training duration\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)\n",
    "\n",
    "# Train model with early stopping\n",
    "model = train_model(model, criterion, optimizer, scheduler, num_epochs=30, patience=5)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"efficientnet_b2_finetuned.pth\")\n",
    "print(\"Model training complete and saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZdOA6vS3lYW0",
    "5AAHAdufuBKg"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (DeepLearning)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
